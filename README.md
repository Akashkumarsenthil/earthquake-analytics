# üåç Earthquake Analytics Pipeline

An end-to-end data analytics platform for USGS earthquake data with real-time ingestion, transformation, and interactive visualization.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   USGS API      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Airflow    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Snowflake     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Streamlit    ‚îÇ
‚îÇ  (GeoJSON Feed) ‚îÇ     ‚îÇ  (DAGs)      ‚îÇ     ‚îÇ  (Warehouse)    ‚îÇ     ‚îÇ  (Dashboard)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ                      ‚îÇ
                              ‚îÇ                      ‚ñº
                              ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ      dbt        ‚îÇ
                                             ‚îÇ (Transformations)‚îÇ
                                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Project Structure

```
earthquake_pipeline/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ earthquake_ingestion.py      # Airflow DAGs for data ingestion
‚îÇ   ‚îî‚îÄ‚îÄ earthquake_dbt_transform.py  # Airflow DAG to run dbt
‚îú‚îÄ‚îÄ dbt/
‚îÇ   ‚îú‚îÄ‚îÄ profiles.yml                 # dbt Snowflake connection
‚îÇ   ‚îî‚îÄ‚îÄ earthquake_analytics/
‚îÇ       ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ       ‚îî‚îÄ‚îÄ models/
‚îÇ           ‚îú‚îÄ‚îÄ staging/
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ sources.yml      # Source definitions
‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ schema.yml       # Model documentation
‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ stg_earthquakes.sql
‚îÇ           ‚îî‚îÄ‚îÄ marts/
‚îÇ               ‚îú‚îÄ‚îÄ schema.yml
‚îÇ               ‚îú‚îÄ‚îÄ fct_earthquakes.sql
‚îÇ               ‚îú‚îÄ‚îÄ agg_daily_summary.sql
‚îÇ               ‚îú‚îÄ‚îÄ agg_regional_risk.sql
‚îÇ               ‚îî‚îÄ‚îÄ agg_hourly_heatmap.sql
‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îî‚îÄ‚îÄ earthquake_dashboard.py      # Streamlit dashboard
‚îú‚îÄ‚îÄ setup/
‚îÇ   ‚îî‚îÄ‚îÄ snowflake_setup.sql          # Snowflake schema setup
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.template
‚îî‚îÄ‚îÄ README.md
```

## Setup Instructions

### 1. Snowflake Setup

1. Log into Snowflake: https://sfedu02-lvb17920.snowflakecomputing.com
2. Run the SQL script in `setup/snowflake_setup.sql` to create schemas and tables
3. Note your credentials:
   - Account: `sfedu02-lvb17920`
   - User: `PLATYPUS`
   - Database: `USER_DB_PLATYPUS`
   - Warehouse: `PLATYPUS_QUERY_WH`

### 2. Environment Setup

```bash
# Copy the env template
cp .env.template .env

# Edit .env with your Snowflake password
nano .env

# Export environment variables
source .env
# Or on Windows: set SNOWFLAKE_PASSWORD=your_password
```

### 3. Airflow Setup

Copy the DAG files to your Airflow dags folder:

```bash
cp dags/*.py ~/DATA226/sjsu-data226-FA25/dags/
```

Set up Airflow Snowflake connection:
1. Go to Airflow UI ‚Üí Admin ‚Üí Connections
2. Add new connection:
   - Connection Id: `snowflake_conn`
   - Connection Type: `Snowflake`
   - Host: `sfedu02-lvb17920`
   - Schema: `RAW`
   - Login: `PLATYPUS`
   - Password: `[your password]`
   - Extra: `{"account": "sfedu02-lvb17920", "warehouse": "PLATYPUS_QUERY_WH", "database": "USER_DB_PLATYPUS", "role": "ACCOUNTADMIN"}`

### 4. dbt Setup

Copy dbt project to your dbt folder:

```bash
cp -r dbt/earthquake_analytics ~/DATA226/sjsu-data226-FA25/dbt/
cp dbt/profiles.yml ~/.dbt/profiles.yml  # Or merge with existing
```

Test dbt connection:

```bash
cd ~/DATA226/sjsu-data226-FA25/dbt/earthquake_analytics
export SNOWFLAKE_PASSWORD=your_password
dbt debug
```

Run dbt models:

```bash
dbt deps
dbt run
dbt test
```

### 5. Dashboard Setup

Install dependencies:

```bash
pip install streamlit pandas plotly snowflake-connector-python
```

Run the dashboard:

```bash
cd dashboard
export SNOWFLAKE_PASSWORD=your_password
streamlit run earthquake_dashboard.py
```

The dashboard will be available at `http://localhost:8501`

## Data Pipeline Flow

### Ingestion (Airflow)

1. **Real-time DAG** (`earthquake_realtime_ingestion`):
   - Runs every 5 minutes
   - Fetches from USGS GeoJSON feed: `https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson`
   - Uses MERGE to handle duplicates (upsert)

2. **Historical Backfill DAG** (`earthquake_historical_backfill`):
   - Manual trigger
   - Fetches from USGS FDSNWS API with date range parameters
   - Use for initial data load or backfilling gaps

### Transformation (dbt)

**Staging Layer:**
- `stg_earthquakes`: Cleans raw data, converts timestamps, adds categories

**Marts Layer:**
- `fct_earthquakes`: Incremental fact table with all earthquake events
- `agg_daily_summary`: Pre-aggregated daily statistics
- `agg_regional_risk`: Risk scores by region
- `agg_hourly_heatmap`: Temporal pattern analysis

### Visualization (Streamlit)

Dashboard features:
- üó∫Ô∏è Interactive map with earthquake locations
- üìà Time-series analysis
- üìä Magnitude distribution
- ‚ö†Ô∏è Regional risk assessment
- üïê Temporal heatmap patterns
- üìã Recent earthquakes table
- üîÑ Auto-refresh option (60s)

## Key Features

| Feature | Description |
|---------|-------------|
| Real-time ingestion | 5-minute refresh from USGS |
| Incremental processing | dbt incremental models for efficiency |
| MERGE upserts | Handles updated earthquake data |
| Risk scoring | Calculated regional risk scores |
| Interactive filters | Date range, magnitude thresholds |
| Auto-refresh | Optional 60-second dashboard refresh |

## Useful Commands

```bash
# Check Airflow DAGs
airflow dags list | grep earthquake

# Trigger historical backfill with parameters
airflow dags trigger earthquake_historical_backfill \
  --conf '{"start_date": "2024-01-01", "end_date": "2024-06-30", "min_magnitude": 3.0}'

# Run specific dbt model
dbt run --select stg_earthquakes
dbt run --select fct_earthquakes

# Run all marts
dbt run --select marts

# Full refresh (rebuild incremental)
dbt run --select fct_earthquakes --full-refresh

# Generate dbt docs
dbt docs generate
dbt docs serve
```

## Troubleshooting

**Snowflake connection issues:**
```bash
# Test connection
python -c "import snowflake.connector; c = snowflake.connector.connect(account='sfedu02-lvb17920', user='PLATYPUS', password='$SNOWFLAKE_PASSWORD'); print('Connected!')"
```

**dbt model failures:**
```bash
# Check compiled SQL
cat target/compiled/earthquake_analytics/models/staging/stg_earthquakes.sql

# Run with debug
dbt run --debug
```

**Airflow task failures:**
- Check Airflow logs in UI
- Verify Snowflake connection in Admin ‚Üí Connections
- Ensure USGS API is accessible

## Live Dashboard

üöÄ **View the live dashboard here**: [Earthquake Analytics Dashboard](https://earthquakedashboardpy-9awudwwqv26znqrc3rpdim.streamlit.app/)

> üåô **Tip**: View the dashboard in **dark mode** for the best experience!
>
> ‚ö†Ô∏è **Note**: We're running this on Streamlit's free cloud tier, so the app may go to sleep after 3 days of inactivity. If the dashboard appears down, please click "Yes, get this app back up!" to restart it.

## Data Sources

- **Real-time Feed**: https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson
- **Historical API**: https://earthquake.usgs.gov/fdsnws/event/1/
- **Documentation**: https://earthquake.usgs.gov/earthquakes/feed/v1.0/geojson.php

---

**Author**: Akash | DATA226 Project | SJSU Fall 2025
